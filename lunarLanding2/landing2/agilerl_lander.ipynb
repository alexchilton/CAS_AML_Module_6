{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:11.243918Z",
     "start_time": "2025-01-22T11:19:11.239816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from agilerl.algorithms.td3 import TD3\n",
    "from agilerl.components.replay_buffer import ReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.training.train_off_policy import train_off_policy\n",
    "from agilerl.utils.utils import create_population, make_vect_envs\n",
    "from tqdm import trange"
   ],
   "id": "8362649b0c58d6b5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:14.344431Z",
     "start_time": "2025-01-22T11:19:14.339278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initial hyperparameters\n",
    "INIT_HP = {\n",
    "    \"ALGO\": \"TD3\",\n",
    "    \"POP_SIZE\": 4,  # Population size\n",
    "    \"BATCH_SIZE\": 128,  # Batch size\n",
    "    \"LR_ACTOR\": 0.0001,  # Actor learning rate\n",
    "    \"LR_CRITIC\": 0.001,  # Critic learning rate\n",
    "    \"O_U_NOISE\": True,  # Ornstein-Uhlenbeck action noise\n",
    "    \"EXPL_NOISE\": 0.1,  # Action noise scale\n",
    "    \"MEAN_NOISE\": 0.0,  # Mean action noise\n",
    "    \"THETA\": 0.15,  # Rate of mean reversion in OU noise\n",
    "    \"DT\": 0.01,  # Timestep for OU noise\n",
    "    \"GAMMA\": 0.99,  # Discount factor\n",
    "    \"MEMORY_SIZE\": 100_000,  # Max memory buffer size\n",
    "    \"POLICY_FREQ\": 2,  # Policy network update frequency\n",
    "    \"LEARN_STEP\": 1,  # Learning frequency\n",
    "    \"TAU\": 0.005,  # For soft update of target parameters\n",
    "    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "    \"CHANNELS_LAST\": False,  # Use with RGB states\n",
    "    \"EPISODES\": 1000,  # Number of episodes to train for\n",
    "    \"EVO_EPOCHS\": 20,  # Evolution frequency, i.e. evolve after every 20 episodes\n",
    "    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n",
    "    \"EVO_LOOP\": 3,  # Number of evaluation episodes\n",
    "    \"MAX_STEPS\": 500,  # Maximum number of steps an agent takes in an environment\n",
    "    \"LEARNING_DELAY\": 1000,  # Steps before starting learning\n",
    "    \"EVO_STEPS\": 10000,  # Evolution frequency\n",
    "    \"EVAL_STEPS\": None,  # Number of evaluation steps per episode\n",
    "    \"EVAL_LOOP\": 1,  # Number of evaluation episodes\n",
    "    \"TOURN_SIZE\": 2,  # Tournament size\n",
    "    \"ELITISM\": True,  # Elitism in tournament selection\n",
    "}\n",
    "\n",
    "# Mutation parameters\n",
    "MUT_P = {\n",
    "    # Mutation probabilities\n",
    "    \"NO_MUT\": 0.4,  # No mutation\n",
    "    \"ARCH_MUT\": 0.2,  # Architecture mutation\n",
    "    \"NEW_LAYER\": 0.2,  # New layer mutation\n",
    "    \"PARAMS_MUT\": 0.2,  # Network parameters mutation\n",
    "    \"ACT_MUT\": 0.2,  # Activation layer mutation\n",
    "    \"RL_HP_MUT\": 0.2,  # Learning HP mutation\n",
    "    # Learning HPs to choose from\n",
    "    \"RL_HP_SELECTION\": [\"lr\", \"batch_size\", \"learn_step\"],\n",
    "    \"MUT_SD\": 0.1,  # Mutation strength\n",
    "    \"RAND_SEED\": 42,  # Random seed\n",
    "    # Define max and min limits for mutating RL hyperparams\n",
    "    \"MIN_LR\": 0.0001,\n",
    "    \"MAX_LR\": 0.01,\n",
    "    \"MIN_BATCH_SIZE\": 8,\n",
    "    \"MAX_BATCH_SIZE\": 1024,\n",
    "    \"MIN_LEARN_STEP\": 1,\n",
    "    \"MAX_LEARN_STEP\": 16,\n",
    "}"
   ],
   "id": "f04f3d0958a66778",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:20.252455Z",
     "start_time": "2025-01-22T11:19:19.081357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_envs=8\n",
    "env = make_vect_envs(\"LunarLanderContinuous-v2\", num_envs=num_envs)  # Create environment\n",
    "try:\n",
    "    state_dim = env.single_observation_space.n, # Discrete observation space\n",
    "    one_hot = True  # Requires one-hot encoding\n",
    "except Exception:\n",
    "    state_dim = env.single_observation_space.shape  # Continuous observation space\n",
    "    one_hot = False  # Does not require one-hot encoding\n",
    "try:\n",
    "    action_dim = env.single_action_space.n  # Discrete action space\n",
    "except Exception:\n",
    "    action_dim = env.single_action_space.shape[0]  # Continuous action space\n",
    "\n",
    "INIT_HP[\"MAX_ACTION\"] = float(env.single_action_space.high[0])\n",
    "INIT_HP[\"MIN_ACTION\"] = float(env.single_action_space.low[0])\n",
    "\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "    # Adjust dimensions for PyTorch API (C, H, W), for envs with RGB image states\n",
    "    state_dim = (state_dim[2], state_dim[0], state_dim[1])"
   ],
   "id": "23dbe2a73df6acec",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:22.302436Z",
     "start_time": "2025-01-22T11:19:22.264170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set-up the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the network configuration of a simple mlp with two hidden layers, each with 64 nodes\n",
    "net_config = {\"arch\": \"mlp\", \"hidden_size\": [64, 64]}\n",
    "\n",
    "# Define a population\n",
    "pop = create_population(\n",
    "    algo=\"TD3\",  # Algorithm\n",
    "    state_dim=state_dim,  # State dimension\n",
    "    action_dim=action_dim,  # Action dimension\n",
    "    one_hot=one_hot,  # One-hot encoding\n",
    "    net_config=net_config,  # Network configuration\n",
    "    INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n",
    "    num_envs=num_envs,\n",
    "    device=device,\n",
    ")"
   ],
   "id": "3a41e7b16a4f79d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:26.320660Z",
     "start_time": "2025-01-22T11:19:26.316632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"terminated\"]\n",
    "memory = ReplayBuffer(\n",
    "    memory_size=10_000,  # Max replay buffer size\n",
    "    field_names=field_names,  # Field names to store in memory\n",
    "    device=device,\n",
    ")"
   ],
   "id": "c5b7fc54f136bd5e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96ea82d8b42adb4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:29.332344Z",
     "start_time": "2025-01-22T11:19:29.329390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tournament = TournamentSelection(\n",
    "    INIT_HP[\"TOURN_SIZE\"],\n",
    "    INIT_HP[\"ELITISM\"],\n",
    "    INIT_HP[\"POP_SIZE\"],\n",
    "    INIT_HP[\"EVAL_LOOP\"],\n",
    ")"
   ],
   "id": "4edc523a11b588c5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b34196447bb904b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:19:31.636699Z",
     "start_time": "2025-01-22T11:19:31.631930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mutations = Mutations(\n",
    "    algo=INIT_HP[\"ALGO\"],\n",
    "    no_mutation=MUT_P[\"NO_MUT\"],\n",
    "    architecture=MUT_P[\"ARCH_MUT\"],\n",
    "    new_layer_prob=MUT_P[\"NEW_LAYER\"],\n",
    "    parameters=MUT_P[\"PARAMS_MUT\"],\n",
    "    activation=MUT_P[\"ACT_MUT\"],\n",
    "    rl_hp=MUT_P[\"RL_HP_MUT\"],\n",
    "    rl_hp_selection=MUT_P[\"RL_HP_SELECTION\"],\n",
    "    min_lr=MUT_P[\"MIN_LR\"],\n",
    "    max_lr=MUT_P[\"MAX_LR\"],\n",
    "    min_batch_size=MUT_P[\"MAX_BATCH_SIZE\"],\n",
    "    max_batch_size=MUT_P[\"MAX_BATCH_SIZE\"],\n",
    "    min_learn_step=MUT_P[\"MIN_LEARN_STEP\"],\n",
    "    max_learn_step=MUT_P[\"MAX_LEARN_STEP\"],\n",
    "    mutation_sd=MUT_P[\"MUT_SD\"],\n",
    "    arch=net_config[\"arch\"],\n",
    "    rand_seed=MUT_P[\"RAND_SEED\"],\n",
    "    device=device,\n",
    ")"
   ],
   "id": "f72c6c3e2eae4094",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d14675cef4428ce2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:21:36.416089Z",
     "start_time": "2025-01-22T11:19:36.449743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_pop, pop_fitnesses = train_off_policy(\n",
    "    env=env,\n",
    "    env_name=\"LunarLanderContinuous-v2\",\n",
    "    algo=\"TD3\",\n",
    "    pop=pop,\n",
    "    memory=memory,\n",
    "    INIT_HP=INIT_HP,\n",
    "    MUT_P=MUT_P,\n",
    "    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "    max_steps=INIT_HP[\"MAX_STEPS\"],\n",
    "    evo_steps=INIT_HP[\"EVO_STEPS\"],\n",
    "    eval_steps=INIT_HP[\"EVAL_STEPS\"],\n",
    "    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n",
    "    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n",
    "    target=INIT_HP[\"TARGET_SCORE\"],\n",
    "    tournament=tournament,\n",
    "    mutation=mutations,\n",
    "    wb=False,  # Boolean flag to record run with Weights & Biases\n",
    "    save_elite=True,  # Boolean flag to save the elite agent in the population\n",
    "    elite_path=\"TD3_trained_agent.pt\",\n",
    ")"
   ],
   "id": "370dba493e478492",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          |    0/500 [  00:00<      ?, ?step/s]\u001B[A\n",
      "|          | 2500/? [  00:27<  00:00, 90.73step/s]  \u001B[A\n",
      "|          | 2500/? [  00:44<  00:00, 90.73step/s]\u001B[A\n",
      "|          | 5000/? [  00:58<  00:00, 85.23step/s]\u001B[A\n",
      "|          | 5000/? [  01:14<  00:00, 85.23step/s]\u001B[A\n",
      "|          | 7500/? [  01:28<  00:00, 83.77step/s]\u001B[A\n",
      "|          | 7500/? [  01:44<  00:00, 83.77step/s]\u001B[A\n",
      "|          | 10000/? [  01:59<  00:00, 83.36step/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                --- Global Steps 40000 ---\n",
      "                Fitness:\t\t['-123.38', '-151.05', '-782.08', '-99.33']\n",
      "                Score:\t\t[-197.25843135491206, -375.8708395298837, -440.64254522825183, -187.08767067966096]\n",
      "                5 fitness avgs:\t['-99.33', '-99.33', '-123.38', '-99.33']\n",
      "                10 score avgs:\t['-95.22', '-95.22', '-176.09', '-95.22']\n",
      "                Agents:\t\t[3, 4, 5, 6]\n",
      "                Steps:\t\t[10000, 10000, 10000, 10000]\n",
      "                Mutations:\t\t['None', 'None', 'param', 'arch']\n",
      "                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "88c2620bcc29eaab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:26:50.034621Z",
     "start_time": "2025-01-22T15:24:31.339673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create fresh environment\n",
    "env = make_vect_envs(\"LunarLanderContinuous-v2\", num_envs=num_envs)\n",
    "\n",
    "# Initialize mutations\n",
    "mutations = Mutations(\n",
    "    algo=\"TD3\",\n",
    "    no_mutation=MUT_P[\"NO_MUT\"],\n",
    "    architecture=MUT_P[\"ARCH_MUT\"],\n",
    "    new_layer_prob=MUT_P[\"NEW_LAYER\"],\n",
    "    parameters=MUT_P[\"PARAMS_MUT\"],\n",
    "    activation=MUT_P[\"ACT_MUT\"],\n",
    "    rl_hp=MUT_P[\"RL_HP_MUT\"],\n",
    "    rl_hp_selection=MUT_P[\"RL_HP_SELECTION\"],\n",
    "    min_lr=MUT_P[\"MIN_LR\"],\n",
    "    max_lr=MUT_P[\"MAX_LR\"],\n",
    "    min_batch_size=MUT_P[\"MIN_BATCH_SIZE\"],\n",
    "    max_batch_size=MUT_P[\"MAX_BATCH_SIZE\"],\n",
    "    min_learn_step=MUT_P[\"MIN_LEARN_STEP\"],\n",
    "    max_learn_step=MUT_P[\"MAX_LEARN_STEP\"],\n",
    "    mutation_sd=MUT_P[\"MUT_SD\"],\n",
    "    arch=net_config[\"arch\"],\n",
    "    rand_seed=MUT_P[\"RAND_SEED\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create new population\n",
    "pop = create_population(\n",
    "    algo=\"TD3\",\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    one_hot=one_hot,\n",
    "    net_config=net_config,\n",
    "    INIT_HP=INIT_HP,\n",
    "    population_size=INIT_HP[\"POP_SIZE\"],\n",
    "    num_envs=num_envs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "# TRAINING LOOP\n",
    "print(\"Training...\")\n",
    "total_training_steps = INIT_HP[\"MAX_STEPS\"] * INIT_HP[\"POP_SIZE\"]\n",
    "print(f\"Total training steps target: {total_training_steps}\")\n",
    "print(f\"Population size: {INIT_HP['POP_SIZE']}\")\n",
    "\n",
    "# Initialize steps for each agent\n",
    "for i in range(len(pop)):\n",
    "    pop[i].steps = [0]\n",
    "\n",
    "print(f\"Initial agent steps: {[agent.steps[-1] for agent in pop]}\")\n",
    "print(f\"MAX_STEPS: {INIT_HP['MAX_STEPS']}\")\n",
    "print(f\"EVO_STEPS: {INIT_HP['EVO_STEPS']}\")\n",
    "print(f\"num_envs: {num_envs}\")\n",
    "\n",
    "pbar = trange(total_training_steps, unit=\"step\")\n",
    "\n",
    "while all(agent.steps[-1] < INIT_HP[\"MAX_STEPS\"] for agent in pop):\n",
    "    print(\"Current steps for each agent:\", [agent.steps[-1] for agent in pop])\n",
    "    print(\"Target MAX_STEPS:\", INIT_HP[\"MAX_STEPS\"])\n",
    "\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "\n",
    "        for idx_step in range(INIT_HP[\"EVO_STEPS\"] // num_envs):\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = np.moveaxis(state, [-1], [-3])\n",
    "\n",
    "            action = agent.get_action(state)  # Get next action from agent\n",
    "\n",
    "            # Act in environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            scores += np.array(reward)\n",
    "            steps += num_envs\n",
    "            total_steps += num_envs\n",
    "\n",
    "            # Collect scores for completed episodes\n",
    "            reset_noise_indices = []\n",
    "            for idx, (d, t) in enumerate(zip(terminated, truncated)):\n",
    "                if d or t:\n",
    "                    completed_episode_scores.append(scores[idx])\n",
    "                    agent.scores.append(scores[idx])\n",
    "                    scores[idx] = 0\n",
    "                    reset_noise_indices.append(idx)\n",
    "            agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "            # Save experience to replay buffer\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                memory.save_to_memory(\n",
    "                    state,\n",
    "                    action,\n",
    "                    reward,\n",
    "                    np.moveaxis(next_state, [-1], [-3]),\n",
    "                    terminated,\n",
    "                    is_vectorised=True,\n",
    "                )\n",
    "            else:\n",
    "                memory.save_to_memory(\n",
    "                    state,\n",
    "                    action,\n",
    "                    reward,\n",
    "                    next_state,\n",
    "                    terminated,\n",
    "                    is_vectorised=True,\n",
    "                )\n",
    "\n",
    "            # Learn according to learning frequency\n",
    "            if memory.counter > INIT_HP[\"LEARNING_DELAY\"] and len(memory) >= agent.batch_size:\n",
    "                for _ in range(num_envs // agent.learn_step):\n",
    "                    # Sample replay buffer\n",
    "                    experiences = memory.sample(agent.batch_size)\n",
    "                    # Learn according to agent's RL algorithm\n",
    "                    agent.learn(experiences)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Update progress bar with actual steps taken\n",
    "        pbar.update(steps)\n",
    "        agent.steps[-1] += steps\n",
    "        pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "    # Evaluate population\n",
    "    fitnesses = [\n",
    "        agent.test(\n",
    "            env,\n",
    "            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "            max_steps=INIT_HP[\"EVAL_STEPS\"],\n",
    "            loop=INIT_HP[\"EVAL_LOOP\"],\n",
    "        )\n",
    "        for agent in pop\n",
    "    ]\n",
    "    mean_scores = [\n",
    "        (\n",
    "            np.mean(episode_scores)\n",
    "            if len(episode_scores) > 0\n",
    "            else \"0 completed episodes\"\n",
    "        )\n",
    "        for episode_scores in pop_episode_scores\n",
    "    ]\n",
    "\n",
    "    print(f\"--- Global steps {total_steps} ---\")\n",
    "    print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n",
    "    print(f\"Scores: {mean_scores}\")\n",
    "    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "    print(\n",
    "        f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n",
    "    )\n",
    "\n",
    "    # Tournament selection and population mutation\n",
    "new_pop = tournament.select(pop)\n",
    "# Convert new_pop into list of agents if needed\n",
    "if isinstance(new_pop[0], list):\n",
    "    new_pop = [agent for sublist in new_pop for agent in sublist]\n",
    "    \n",
    "    # Copy over the steps from old population to new population\n",
    "    for i in range(len(new_pop)):\n",
    "        if not hasattr(new_pop[i], 'steps'):\n",
    "            new_pop[i].steps = []\n",
    "        new_pop[i].steps = pop[i].steps.copy()\n",
    "    pop = new_pop\n",
    "    \n",
    "    # Apply mutations\n",
    "    pop = mutations.mutation(pop)\n",
    "    \n",
    "    # Update step counter\n",
    "    for agent in pop:\n",
    "        if not hasattr(agent, 'steps'):\n",
    "            agent.steps = [0]\n",
    "        agent.steps.append(agent.steps[-1])\n",
    "\n",
    "# Save the trained algorithm\n",
    "save_path = \"TD3_trained_agent.pt\"\n",
    "best_agent = max(pop, key=lambda agent: np.mean(agent.fitness[-5:]))\n",
    "best_agent.save_checkpoint(save_path)\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ],
   "id": "7cd5445d79d02313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Total training steps target: 2000\n",
      "Population size: 4\n",
      "Initial agent steps: [0, 0, 0, 0]\n",
      "MAX_STEPS: 500\n",
      "EVO_STEPS: 10000\n",
      "num_envs: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "40000step [08:32, 77.98step/s] ?, ?step/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current steps for each agent: [0, 0, 0, 0]\n",
      "Target MAX_STEPS: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "10000step [00:32, 308.90step/s]           \u001B[A\n",
      "10000step [00:50, 308.90step/s]\u001B[A\n",
      "20000step [01:07, 293.10step/s]\u001B[A\n",
      "20000step [01:20, 293.10step/s]\u001B[A\n",
      "30000step [01:41, 294.27step/s]\u001B[A\n",
      "30000step [02:00, 294.27step/s]\u001B[A\n",
      "40000step [02:17, 291.32step/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global steps 40000 ---\n",
      "Steps [10000, 10000, 10000, 10000]\n",
      "Scores: [-151.86800484035982, -445.20700107674355, -179.95554411281196, -382.4602919634159]\n",
      "Fitnesses: ['-108.76', '-256.85', '-241.07', '-116.51']\n",
      "5 fitness avgs: ['-108.76', '-256.85', '-241.07', '-116.51']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:28:03.886318Z",
     "start_time": "2025-01-22T15:28:03.854046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from agilerl.algorithms.td3 import TD3\n",
    "\n",
    "# Create TD3 agent with same configuration used during training\n",
    "td3 = TD3(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    one_hot=one_hot,           # Added one_hot parameter\n",
    "    lr_actor=INIT_HP[\"LR_ACTOR\"],\n",
    "    lr_critic=INIT_HP[\"LR_CRITIC\"],\n",
    "    gamma=INIT_HP[\"GAMMA\"],\n",
    "    tau=INIT_HP[\"TAU\"],\n",
    "    batch_size=INIT_HP[\"BATCH_SIZE\"],\n",
    "    policy_freq=INIT_HP[\"POLICY_FREQ\"],\n",
    "    net_config=net_config,\n",
    "    device=device,\n",
    "    max_action=INIT_HP[\"MAX_ACTION\"],\n",
    "    min_action=INIT_HP[\"MIN_ACTION\"]\n",
    ")\n",
    "\n",
    "# Load the trained weights\n",
    "td3.load_checkpoint(path=\"TD3_trained_agent.pt\")"
   ],
   "id": "7da7e8f3622315ea",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:28:10.086365Z",
     "start_time": "2025-01-22T15:28:07.869587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_env = gym.make(\"LunarLanderContinuous-v2\", render_mode=\"rgb_array\")\n",
    "rewards = []\n",
    "frames = []\n",
    "testing_eps = 7\n",
    "max_testing_steps = 1000\n",
    "with torch.no_grad():\n",
    "    for ep in range(testing_eps):\n",
    "        state = test_env.reset()[0]  # Reset environment at start of episode\n",
    "        score = 0\n",
    "\n",
    "        for step in range(max_testing_steps):\n",
    "            # If your state is an RGB image\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = np.moveaxis(state, [-1], [-3])\n",
    "\n",
    "            # Get next action from agent\n",
    "            action, *_ = td3.get_action(state, training=False)\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = test_env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            # Take the action in the environment\n",
    "            state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "            # Collect the score\n",
    "            score += reward\n",
    "\n",
    "            # Break if environment 0 is done or truncated\n",
    "            if terminated or truncated:\n",
    "                print(\"terminated\")\n",
    "                break\n",
    "\n",
    "        # Collect and print episodic reward\n",
    "        rewards.append(score)\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "\n",
    "    print(rewards)\n",
    "\n",
    "    test_env.close()\n"
   ],
   "id": "ec12cd13aae0c457",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated\n",
      "--------------- Episode: 0 ---------------\n",
      "Episodic Reward:  -74.20756932186471\n",
      "terminated\n",
      "--------------- Episode: 1 ---------------\n",
      "Episodic Reward:  -55.38149064423747\n",
      "terminated\n",
      "--------------- Episode: 2 ---------------\n",
      "Episodic Reward:  -51.0663876123224\n",
      "terminated\n",
      "--------------- Episode: 3 ---------------\n",
      "Episodic Reward:  -115.23420822332933\n",
      "terminated\n",
      "--------------- Episode: 4 ---------------\n",
      "Episodic Reward:  -68.60284897985841\n",
      "terminated\n",
      "--------------- Episode: 5 ---------------\n",
      "Episodic Reward:  -79.72184345724348\n",
      "terminated\n",
      "--------------- Episode: 6 ---------------\n",
      "Episodic Reward:  -60.27319016621098\n",
      "[-74.20756932186471, -55.38149064423747, -51.0663876123224, -115.23420822332933, -68.60284897985841, -79.72184345724348, -60.27319016621098]\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:28:50.112146Z",
     "start_time": "2025-01-22T15:28:47.887737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frames = frames[::3]\n",
    "gif_path = \"./videos/\"\n",
    "os.makedirs(gif_path, exist_ok=True)\n",
    "imageio.mimwrite(\n",
    "    os.path.join(\"./videos/\", \"td3_lunar_lander.gif\"), frames, duration=50, loop=0\n",
    ")\n",
    "mean_fitness = np.mean(rewards)"
   ],
   "id": "4ea504a7e42c9e67",
   "outputs": [],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
