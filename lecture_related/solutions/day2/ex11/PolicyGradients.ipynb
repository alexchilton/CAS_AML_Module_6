{"cells":[{"cell_type":"markdown","metadata":{"id":"BzPXuZ3WIYMf"},"source":["# Exercise 11) Policy Gradients"]},{"cell_type":"markdown","metadata":{"id":"ybb75V9UIYMm"},"source":["In this exercise we will have a look at policy gradients.\n","The theory of policy gradients applies to function approximators that decide on which action to choose.\n","The function approximators we met in the past were employed to estimate the (action) value function.\n","Since their task was to judge the quality of the current situation they are often referred to as \"critics\".\n","In contrary, we can also use a function approximator to directly choose an action; we call these \"actors\".\n","Why should we do that if we made it work with nothing more than a critic?\n","Because this will finally allow us to make use of contiuous action spaces! Eureka!\n","\n","In this exercise we will use a new `gym` environment `LunarLanderContinuous-v2`.\n","To run this environment please make sure to have `Box2D` installed: `pip install Box2D`.\n","\n","![](https://images.squarespace-cdn.com/content/v1/59e0d6f0197aea1a0abc8016/1507938542206-41S6K9T97YETKEHP0PQF/ke17ZwdGBToddI8pDm48kMR1yAHb8bPoH1-OdajP2rZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyDg3tXaPHS4cFkn9Bnm-emI0BDr_E-XKAFKqWrx68ZVlLyhCgVi_FJvVMH7mHrc18/lunar_lander_success_example.gif?format=500w)\n","\n","Source: https://www.billyvreeland.com/portfolio/2017/1013/solving-openai-gym-nm4yz\n","\n","The main task is to land the LunarLander in the landing zone.\n","An accident-free landing is defined by both legs coming into  ground contact with moderate velocity.\n","We are dealing with a continuous state and action space as defined below.\n","Please notice that the control functions for main and side engines contain a dead zone in which the engines are inactive.\n","The reward is mainly defined depending on whether the landing procedure is successful (+100) or not (-100).\n","Firing the main engine gives a small negative reward.\n","The problem is solved if a return of at least 200 is reached.\n","For more information see https://gymnasium.farama.org/environments/box2d/lunar_lander/.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sPf1gz0TIYMq"},"source":["\n","\\begin{align}\n","\\text{state}\u0026=\n","\\begin{bmatrix}\n","p_x\\\\\n","p_y\\\\\n","v_x\\\\\n","v_y\\\\\n","\\varphi\\\\\n","20 \\, \\omega\\\\\n","1 \\text{ if left leg has ground contact, else } 0\\\\\n","1 \\text{ if right leg has ground contact, else } 0\\\\\n","\\end{bmatrix}\n","\\\\\n","\\text{action}\u0026=\n","\\begin{bmatrix}\n","\\text{main engine: } [-1, 0] \\rightarrow \\text{off}, ]0, 1] \\rightarrow [50 \\, \\%, 100 \\, \\%] \\text{ of available power}\\\\\n","\\text{side engines: } [-1, -0.5] \\rightarrow [50 \\, \\%, 100 \\, \\%] \\text{ of available right engine power}, [0.5, 1] \\rightarrow [50 \\, \\%, 100 \\, \\%] \\text{ of available left engine power}\\\\\n","\\end{bmatrix}\n","\\end{align}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41778,"status":"ok","timestamp":1737132024946,"user":{"displayName":"Andre Bodmer","userId":"14573247772902797623"},"user_tz":-60},"id":"20QZ3R79IYMs","outputId":"075c2c18-2ded-4fc8-dee6-333e6d3ea7b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Box2D\n","  Downloading Box2D-2.3.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n","Downloading Box2D-2.3.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Box2D\n","Successfully installed Box2D-2.3.10\n","Collecting gymnasium\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy\u003e=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle\u003e=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions\u003e=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications\u003e=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"]}],"source":["!pip install Box2D\n","!pip install gymnasium\n","\n","import numpy as np\n","import pandas as pd\n","import gymnasium as gym\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.kernel_approximation import RBFSampler\n","import sklearn.pipeline\n","import sklearn.preprocessing\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfE-9Va5IYMu"},"outputs":[],"source":["env = gym.make(\"LunarLander-v3\",\n","                continuous=True,\n","              render_mode = \"human\")\n","\n","state = env.reset()\n","while True:\n","    state, reward, terminated, truncated, _ = env.step(env.action_space.sample())\n","    done = terminated or truncated\n","\n","    if done:\n","        break\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"O3jB8_4LIYMv"},"source":["## 1) Monte Carlo Policy Gradient\n","Write a REINFORCE algorithm."]},{"cell_type":"markdown","metadata":{"id":"6FkIoDpkIYMx"},"source":["Execute the following cell to fit the featurizer using RBFSampler, like already learned in the last exercises."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"elapsed":31150,"status":"ok","timestamp":1737132111709,"user":{"displayName":"Andre Bodmer","userId":"14573247772902797623"},"user_tz":-60},"id":"wAD6jwoUIYMy","outputId":"9fc89f52-6549-48cd-a7a0-db4c9b33fdd9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"130699bd63154e65bcafb5c7681eb682","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["env = gym.make(\"LunarLander-v3\",\n","                continuous=True)\n","state_array = []\n","state = env.reset()\n","\n","for i in tqdm(range(1000)):\n","    state, _ = env.reset()\n","    while True:\n","        state, reward, terminated, truncated, _ = env.step(env.action_space.sample())\n","        done = terminated or truncated\n","        state_array.append(state)\n","\n","        if done:\n","            break\n","\n","state_array = np.array(state_array)\n","\n","featurizer = sklearn.pipeline.make_pipeline(\n","    sklearn.preprocessing.StandardScaler(),\n","    sklearn.pipeline.FeatureUnion([\n","    (\"rbf0\", RBFSampler(gamma=5.0, n_components = 1000)),\n","    (\"rbf1\", RBFSampler(gamma=2.0, n_components = 1000)),\n","    (\"rbf2\", RBFSampler(gamma=1.0, n_components = 1000)),\n","    (\"rbf3\", RBFSampler(gamma=0.5, n_components = 1000)),\n","    ]),\n","    sklearn.preprocessing.StandardScaler()\n",")\n","\n","_ = featurizer.fit(state_array)\n","env.close()\n"]},{"cell_type":"markdown","metadata":{"id":"SIIxIoh3IYM0"},"source":["Use the following cell to define the function approximators for the policy.\n","As seen in Algo.12.1 we need to calculate $\\nabla_\\theta \\mathrm{ln}\\,\\pi(u_k | x_k, \\theta)$.\n","$\\pi$ is herein defined as the normal distribution :\n","\\begin{align}\n","\\pi(u_k | x_k, \\theta) \u0026 = \\frac{\\mathrm{exp} \\left( {-\\frac{1}{2} (u_k - \\mu_\\theta(x_k))^\\mathrm{T} \\mathbf{\\Sigma}^{-1}_\\theta(x_k) (u_k - \\mu_\\theta(x_k))} \\right)}{\\sqrt{(2\\pi)^p \\mathrm{det}(\\mathbf{\\Sigma}_\\theta(x_k))}},\\\\\n","\\text{with}\\hspace{1em} p \u0026 = \\mathrm{dim}(u_k).\n","\\end{align}\n","\n","Extend `loglikelyhoodGaussian` such that it returns $\\mathrm{ln}\\,\\pi(u_k | x_k, \\theta)$!\n","Use the numpy equivalent `PyTorch` functions (e.g. `torch.linalg.inv()`).\n","`PyTorch` functions are differentiable and can therefore be  used to calculate $\\nabla_\\theta \\mathrm{ln}\\,\\pi(u_k | x_k, \\theta)$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2u_V-1fIYM2"},"outputs":[],"source":["class PolicyNetwork(nn.Module):\n","\n","    def __init__(self, input_dim, action_space_dim):\n","        super(PolicyNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 400)\n","        self.fc2 = nn.Linear(400, 400)\n","        self.fc3_mu = nn.Linear(400, action_space_dim)\n","        self.fc3_sigma = nn.Linear(400, action_space_dim)\n","\n","    def forward(self, x):\n","        \"\"\"Predict the parameters of the stochastic distribution from which the action\n","        is sampled.\n","        \"\"\"\n","        x = F.leaky_relu(self.fc1(x), 0.1)\n","        x = F.leaky_relu(self.fc2(x), 0.1)\n","\n","        mu_out = self.fc3_mu(x)\n","        mu_out = torch.clip(mu_out, -1, 1)[0]\n","\n","        sigma_out = F.softplus(self.fc3_sigma(x))\n","        sigma_out = torch.mm(sigma_out, torch.tensor([[0.01, 0], [0, 0.1]]))\n","        sigma_out = torch.clip(sigma_out, 1e-4, 1)\n","        sigma_out = torch.diag_embed(sigma_out[0])\n","\n","        return mu_out, sigma_out\n","\n","\n","def loglikelihood_gaussian(u, arg_mu, arg_sigma):\n","    \"\"\"Evaluate the loglikelihood of the multivariate gaussian.\n","\n","    Args:\n","        u: Where to evaluate the loglikelihood of the gaussian. In our case, this is\n","            the action whose likelihood we are evaluating.\n","        arg_mu: The mean vector of the gaussian distribution\n","        arg_sigma: The covariance matrix of the gaussian distribution\n","    \"\"\"\n","    u = u.float()\n","    arg_mu = arg_mu.float()\n","    arg_sigma = arg_sigma.float()\n","\n","    u_min_mu = u - arg_mu\n","    u_min_mu_col = u_min_mu.unsqueeze(1)\n","\n","    quadratic_term = u_min_mu_col.t().mm(torch.inverse(arg_sigma)).mm(u_min_mu_col)\n","    log_det_sigma = torch.logdet(arg_sigma)\n","\n","    dim = u.size(0)\n","    constant_term = dim * torch.log(torch.tensor(2.0 * np.pi))\n","    log_likelihood = -0.5 * (constant_term + log_det_sigma + quadratic_term)\n","\n","    return log_likelihood.squeeze()  # Ensure it's a scalar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpVEQDMVIYM3"},"outputs":[],"source":["env = gym.make(\"LunarLander-v3\", continuous=True)\n","\n","state = np.reshape(env.reset()[0], (1, -1))\n","feature_state = featurizer.transform(state)\n","input_dim = feature_state.shape[1]\n","action_space_dim = len(env.action_space.sample())\n","\n","policy = PolicyNetwork(input_dim, action_space_dim)\n","\n","# Regularization; downscaling of the network parameters to prevent divergence\n","with torch.no_grad():\n","    for param in policy.parameters():\n","        param.mul_(0.4)"]},{"cell_type":"markdown","metadata":{"id":"rW9iwwCkIYM3"},"source":["Use the following template to write a REINFORCE algorithm.\n","This time the Adam (adaptive moment estimation) optimizer is used, which is an enhanced SGD optimizer.\n","For more information on the optimizer, see https://arxiv.org/abs/1412.6980."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tU_KIAoqIYM4"},"outputs":[],"source":["def interact(state, policy, deterministic, featurizer):\n","    \"\"\"Interact with the environment to get to the next state.\n","\n","    Args:\n","        state: The current state of the environment\n","        policy: The model that returns the mean and covariance matrix of the policy distribution\n","        deterministic: Whether to choose actions randomly or use the mean\n","        featurizer: The featurizer for state preprocessing\n","\n","    Returns:\n","        next_state: The following state of the environment after the interaction\n","        reward: The reward for the interaction\n","        done: Whether the current episode is finished\n","        action: The chosen action\n","        mu: The mean vector of the policy distribution\n","        sigma: The covariance matrix of the policy distribution\n","    \"\"\"\n","    feature_state = torch.tensor(featurizer.transform(state), dtype=torch.float32)\n","    mu, sigma = policy(feature_state)\n","\n","    if deterministic:\n","        action = mu.detach().numpy().ravel()\n","    else:\n","        action = np.random.multivariate_normal(mean=mu.detach().numpy().ravel(), cov=sigma.detach().numpy())\n","\n","    next_state, reward, terminated, truncated, _ = env.step(action)\n","    done = terminated or truncated\n","\n","    return next_state, reward, done, action, mu, sigma\n","\n","def gather_experience(policy, featurizer, max_episode_len=1000):\n","    \"\"\"Simulates a full episode and returns the gathered data.\n","\n","    Args:\n","        policy: The model that returns the mean and covariance matrix of the policy distribution\n","        featurizer: The featurizer for state preprocessing\n","        max_episode_len: The number of steps at which the episode is terminated automatically\n","\n","    Returns:\n","        states: The states visited in the episode\n","        actions: The actions applied in the epsiode\n","        rewards: The rewards gathered in the episode\n","        probs_log: The loglikelihood values for the chosen actions\n","        accumulated_rewards: The sum of rewards over the episode\n","    \"\"\"\n","    accumulated_rewards = 0\n","\n","    states = []\n","    actions = []\n","    rewards = []\n","    probs_log = []\n","\n","    state, _ = env.reset()\n","    state = torch.tensor(state.reshape(1, -1), dtype=torch.float32)\n","\n","    for _ in range(max_episode_len):  # Limiting each episode to a maximum length\n","        next_state, reward, done, action, mu, sigma = interact(state, policy, False, featurizer)\n","\n","        states.append(state)\n","        actions.append(torch.tensor(action, dtype=torch.float32))\n","        rewards.append(reward)\n","        probs_log.append(loglikelihood_gaussian(torch.tensor(action, dtype=torch.float32), mu, sigma))\n","\n","        accumulated_rewards += reward\n","        state = torch.tensor(next_state.reshape(1, -1), dtype=torch.float32)\n","\n","        if done:\n","            break\n","\n","    return states, actions, rewards, probs_log, accumulated_rewards\n","\n","\n","def compute_returns(states, rewards, gamma):\n","    \"\"\"Compute the returns based on the gathered rewards.\"\"\"\n","    g_returns = []\n","    for t in range(len(states)):\n","        g_t = 0\n","        pw = 0\n","        for r in rewards[t:]:\n","            g_t = g_t + gamma**pw * r\n","            pw = pw + 1\n","        #NOTE: The following line is closer to the lecture pseudocode, but it seems to be working worse\n","        #g_returns.append(g_t*(gamma**t))\n","        g_returns.append(g_t)\n","    return torch.tensor(g_returns, dtype=torch.float32)\n","\n","\n","def learn(states, rewards, gamma, probs_log, optimizer):\n","    \"\"\"Learn from the gathered experience.\"\"\"\n","\n","    g_returns = compute_returns(states, rewards, gamma)\n","\n","    # Calculate Policy Loss\n","    policy_loss = 0\n","    for log_prob, g_t in zip(probs_log, g_returns):\n","        policy_loss = policy_loss - log_prob * g_t\n","\n","    # Update Policy\n","    optimizer.zero_grad()\n","    policy_loss.backward()\n","    optimizer.step()\n","\n","    return policy_loss.item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"yamvdLf5IYM8","scrolled":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ce6e620d6f145d29ae7ab367c81bcb5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-0d77ab3acf07\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulated_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgather_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-6-861936329ac2\u003e\u001b[0m in \u001b[0;36mgather_experience\u001b[0;34m(policy, featurizer, max_episode_len)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episode_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Limiting each episode to a maximum length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 57\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-6-861936329ac2\u003e\u001b[0m in \u001b[0;36minteract\u001b[0;34m(state, policy, deterministic, featurizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0mfeature_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 19\u001b[0;31m     \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-4-f6f3938493e3\u003e\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mis\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \"\"\"\n\u001b[0;32m---\u003e 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n","return_history = []\n","\n","alpha_policy = 1e-5\n","optimizer = torch.optim.Adam(policy.parameters(), lr=alpha_policy)\n","gamma = 0.99\n","nb_episodes = 5000\n","\n","for j in tqdm(range(nb_episodes)):\n","\n","    states, actions, rewards, probs_log, accumulated_rewards = gather_experience(policy, featurizer)\n","\n","    _ = learn(states, rewards, gamma, probs_log, optimizer)\n","\n","    if j % 250 == 0 and j != 0:\n","        plt.plot(return_history, label='Return')\n","        plt.plot(pd.Series(return_history).rolling(10).mean(), label='MA')\n","        plt.xlabel('episode')\n","        plt.ylabel('return')\n","        plt.grid(True)\n","        _ = plt.legend()\n","        plt.show()\n","    return_history.append(accumulated_rewards)\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"ze8HuDJ-IYM9"},"source":["Plot the learning curve of the training process!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E36fHSG1IYM-"},"outputs":[],"source":["plt.plot(return_history, label='Return')\n","plt.plot(pd.Series(return_history, name='reward_history').rolling(10).mean(), label='MA')\n","plt.xlabel('episode')\n","plt.ylabel('return')\n","plt.grid(True)\n","_=plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"5ktES8JaIYM_"},"source":["### Execution\n","\n","Use `deterministic` to choose between deterministic execution (applying $\\mu$ directly) or taking the stochastic action by sampling from the normal distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf83lgfYIYNA","scrolled":true},"outputs":[],"source":["env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"human\")\n","\n","deterministic = False\n","\n","for j in tqdm(range(10)):\n","    state, _ = env.reset()\n","    accumulated_rewards = 0\n","\n","    while True:\n","        with torch.no_grad():\n","            next_state, reward, done, action, mu, sigma = interact(\n","                state = torch.tensor(state.reshape(1, -1), dtype=torch.float32),\n","                policy=policy,\n","                deterministic=deterministic,\n","                featurizer=featurizer\n","            )\n","\n","        accumulated_rewards += reward\n","        state = next_state\n","\n","        if done:\n","            print(f\"Episode {j}, Total Reward: {accumulated_rewards}\")\n","            break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"wsudu4fQIYNC"},"source":["## 2) Actor-Critic with TD(0) Targets\n","\n","Write an actor-critic (AC) algorithm to land the lander in the landing zone.\n"]},{"cell_type":"markdown","metadata":{"id":"JyoMP7ZSIYNC"},"source":["Use the following cell to create two function approximators. One to estimate the state values (critic) and one to decide on the actions to take (actor)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUlZrhNiIYND"},"outputs":[],"source":["env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n","\n","state = np.reshape(env.reset()[0], (1, -1))\n","input_dim = len(featurizer.transform(state)[0])\n","action_space_dim = len(env.action_space.sample())\n","\n","class CriticNetwork(nn.Module):\n","    def __init__(self, input_dim):\n","        super(CriticNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 400)\n","        self.fc2 = nn.Linear(400, 400)\n","        self.fc3 = nn.Linear(400, 1)\n","\n","    def forward(self, x):\n","        x = F.leaky_relu(self.fc1(x), 0.1)\n","        x = F.leaky_relu(self.fc2(x), 0.1)\n","        x = self.fc3(x)\n","        return x\n","\n","critic = CriticNetwork(input_dim)\n","\n","for param in critic.parameters():\n","    param.data.mul_(0.2)\n","\n","actor = PolicyNetwork(input_dim, action_space_dim)\n","\n","for param in actor.parameters():\n","    param.data.mul_(0.4)\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"PWS5ig3cIYNE"},"source":["Use the following template to write an AC algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lZpQjLRIYNE"},"outputs":[],"source":["def learn_critic(critic, critic_optimizer, state, next_state, done, gamma, featurizer):\n","    \"\"\"Updates the critic network based on the provided states and rewards.\n","\n","    Args:\n","        critic: The critic network to be updated\n","        critic_optimizer: The optimizer for updating the critic network\n","        state: The current state\n","        next_state: The next state\n","        done: Flag indicating whether the episode is finished\n","        gamma: The discount factor for future rewards\n","        featurizer: The featurizer for state preprocessing\n","\n","    Returns:\n","        delta: The temporal difference error for the current state\n","        critic_loss.item(): The value of the critic loss function\n","    \"\"\"\n","    feat_state = torch.tensor(featurizer.transform(np.reshape(state, (1, -1))), dtype=torch.float32)\n","    next_feat_state = torch.tensor(featurizer.transform(np.reshape(next_state, (1, -1))), dtype=torch.float32)\n","\n","    this_value = critic(feat_state)\n","    with torch.no_grad():\n","        next_value = critic(next_feat_state)\n","        delta = reward + (gamma * next_value * (1 - int(done))) - this_value\n","\n","    critic_loss = - delta * this_value\n","    critic_optimizer.zero_grad()\n","    critic_loss.backward()\n","    critic_optimizer.step()\n","\n","    return delta, critic_loss.item()\n","\n","\n","def learn_actor(actor_optimizer, action, mu, sigma, delta, gamma_k):\n","    \"\"\"Updates the actor network based on the provided action, mean, and standard deviation.\n","\n","    Args:\n","        actor_optimizer: The optimizer for updating the actor network\n","        action: The action taken\n","        mu: The mean of the action distribution\n","        sigma: The standard deviation of the action distribution\n","        delta: The temporal difference error for the current state\n","        gamma_k: Importance sampling weight for updating the actor (gamma**k)\n","\n","    Returns:\n","        actor_loss.item(): The value of the actor loss function\n","    \"\"\"\n","\n","    actor_loss = - loglikelihood_gaussian(torch.tensor(action, dtype=torch.float32), mu, sigma) * gamma_k * delta\n","\n","    actor_optimizer.zero_grad()\n","    actor_loss.backward()\n","    actor_optimizer.step()\n","\n","    return actor_loss.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-i6eEpE2IYNF"},"outputs":[],"source":["env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n","return_history = []\n","\n","alpha_critic = 1e-4\n","alpha_actor = 1e-5\n","gamma = 0.99\n","nb_episodes = 500\n","max_episode_len = 1000\n","\n","critic_optimizer = torch.optim.Adam(critic.parameters(), lr=alpha_critic)\n","actor_optimizer = torch.optim.Adam(actor.parameters(), lr=alpha_actor)\n","\n","for j in tqdm(range(nb_episodes)):\n","\n","    accumulated_rewards = 0\n","    state, _ = env.reset()\n","    gamma_k = 1\n","\n","    for _ in range(max_episode_len):\n","\n","        next_state, reward, done, action, mu, sigma = interact(\n","            state=np.reshape(state, (1, -1)),\n","            policy=actor,\n","            deterministic=False,\n","            featurizer=featurizer\n","        )\n","        accumulated_rewards += reward\n","\n","        delta, _ = learn_critic(critic, critic_optimizer, state, next_state, done, gamma, featurizer)\n","        _ = learn_actor(actor_optimizer, action, mu, sigma, delta, gamma_k)\n","\n","        gamma_k = gamma * gamma_k\n","        state = next_state\n","\n","        if done:\n","            break\n","\n","    if j % 100 == 0 and j != 0:\n","        plt.plot(return_history, label='Return')\n","        plt.plot(pd.Series(return_history).rolling(10).mean(), label='MA')\n","        plt.xlabel('episode')\n","        plt.ylabel('return')\n","        plt.grid(True)\n","        _ = plt.legend()\n","        plt.show()\n","    return_history.append(accumulated_rewards)\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"6JNCJq3eIYNH"},"source":["Plot the learning curve of the training process!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6MXhVqdIYNH"},"outputs":[],"source":["plt.plot(return_history, label='Return')\n","plt.plot(pd.Series(return_history, name='reward_history').rolling(10).mean(), label='MA')\n","plt.xlabel('episode')\n","plt.ylabel('return')\n","plt.grid(True)\n","_=plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"qnFRtK6KIYNH"},"source":["## Execution\n","\n","Use `deterministic` to choose between deterministic execution (apply $\\mu$) directly or take the stochastic action by sampling from the normal distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tlHLEpjIYNI"},"outputs":[],"source":["env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"human\")\n","deterministic = True\n","\n","for j in tqdm(range(10)):\n","    state, _ = env.reset()\n","    accumulated_rewards = 0\n","\n","    while True:\n","\n","        with torch.no_grad():\n","\n","            next_state, reward, done, action, mu, sigma = interact(\n","                state=np.reshape(next_state, (1, -1)),\n","                policy=actor,\n","                deterministic=deterministic,\n","                featurizer=featurizer\n","            )\n","\n","        accumulated_rewards += reward\n","        state = next_state\n","\n","        if done:\n","            print(f\"Episode {j}, Total Reward: {accumulated_rewards}\")\n","            break\n","\n","env.close()"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0f59d220b294462b9b6e04066cd97a76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"130699bd63154e65bcafb5c7681eb682":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_346a7308b89c4e47bb2facf15d40ea13","IPY_MODEL_a84770f70db64010a99ebf49a9a96e88","IPY_MODEL_694db71691d6426bba0203eaecd90a91"],"layout":"IPY_MODEL_df232a5aa89d4556a83cc49ccbadf952"}},"1bc9583a0cae47a483f9705b9d029726":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"346a7308b89c4e47bb2facf15d40ea13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bc9583a0cae47a483f9705b9d029726","placeholder":"​","style":"IPY_MODEL_6fceb73dce88489e851712bf943d4e68","value":"100%"}},"48ea64945d434cde8e16bfcefdd5d90d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e36081fcd09048feb0d5c9eb2bc3472f","IPY_MODEL_52c76c2581774528a6db592d3212ff53","IPY_MODEL_c77a3407107349c4bfc86fb6af9266b0"],"layout":"IPY_MODEL_89282f37a160449abd779503f503282a"}},"4e9beb522c58400299a3494915849331":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52c76c2581774528a6db592d3212ff53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7823f53afc104c79be554634e09e5476","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72e2fae9116b4d8092a3dbbb2c0255a2","value":2016}},"59f5093a6b674e72a59d47129182cf92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"694db71691d6426bba0203eaecd90a91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e9beb522c58400299a3494915849331","placeholder":"​","style":"IPY_MODEL_793a079227774f9b959fbe369a406e9f","value":" 1000/1000 [00:22\u0026lt;00:00, 56.78it/s]"}},"6fceb73dce88489e851712bf943d4e68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72e2fae9116b4d8092a3dbbb2c0255a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7823f53afc104c79be554634e09e5476":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"793a079227774f9b959fbe369a406e9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89282f37a160449abd779503f503282a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"917fd064442a4101950b29e687ed5536":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f1b1b1295b74a8696a5efbf002c775c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a84770f70db64010a99ebf49a9a96e88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_917fd064442a4101950b29e687ed5536","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59f5093a6b674e72a59d47129182cf92","value":1000}},"bb47309d558943959136dcd94fcc851a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c77a3407107349c4bfc86fb6af9266b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f59d220b294462b9b6e04066cd97a76","placeholder":"​","style":"IPY_MODEL_9f1b1b1295b74a8696a5efbf002c775c","value":" 2016/5000 [34:52\u0026lt;47:18,  1.05it/s]"}},"df232a5aa89d4556a83cc49ccbadf952":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e36081fcd09048feb0d5c9eb2bc3472f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb47309d558943959136dcd94fcc851a","placeholder":"​","style":"IPY_MODEL_f81286d369004142a297b39c32ab2246","value":" 40%"}},"f81286d369004142a297b39c32ab2246":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}